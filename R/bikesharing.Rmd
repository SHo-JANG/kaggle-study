---
title: "Untitled"
output: html_document
---

```{r, include=F, message=F}
library(tidymodels)
library(tidyverse)
library(lubridate)
library(skimr)
library(magrittr)
library(data.table)
library(gridExtra)

theme_set(theme_bw())
```

```{r}
train <- read_csv("C:/Users/uos/Desktop/kaggle-study/data/bike sharing/train.csv")
test <- read_csv("C:/Users/uos/Desktop/kaggle-study/data/bike sharing/test.csv")
```

```{r}
head(train)
names(train)
names(test)
```

```{r}
train %>% 
    select(-c(casual, registered)) -> train
```

```{r}
all_data <- bind_rows(train, test)
tail(all_data)

glimpse(all_data)
all_data$holiday %>% unique()
all_data$workingday %>% unique()
all_data$weather %>% unique()

all_data$season <- factor(all_data$season, labels = c('spring', 'summer', 'fall', 'winter'))
all_data$weather <- as.factor(all_data$weather)
all_data$workingday <- as.factor(all_data$workingday)
all_data$holiday <- as.factor(all_data$holiday)

all_data %>% mutate(year = year(datetime), 
                    month = month(datetime),
                    wday = wday(datetime),
                    day = day(datetime), 
                    hour = hour(datetime)) %>% 
    select(year, month, wday, day, holiday, workingday, everything()) -> all_data

all_data$wday <- factor(all_data$wday, labels = c('Sun', 'Mon', 'Tue', 'Wed', 'Thur', 'Fri', 'Sat'))
all_data
```


```{r}
skim(all_data)

```

```{r}

all_data %>% 
    ggplot(aes(x = count)) + 
    geom_histogram()
p1 <- all_data %>% 
    ggplot(aes(x = atemp)) + 
    geom_histogram()
p2 <- all_data %>% 
    ggplot(aes(x = temp)) + 
    geom_histogram()

grid.arrange(grobs = list(p1, p2), col = 2)

all_data %>% 
    select(holiday, temp, humidity, windspeed, count) %>% 
    GGally::ggpairs(mapping = aes(color = holiday))

all_data %>% 
    select(workingday, temp, humidity, windspeed, count) %>% 
    GGally::ggpairs(mapping = aes(color = workingday))
```
```{r}
factor_list <- c('holiday', 'workingday', 'season', 'weather', 'wday')


all_data %>% 
    group_by(season, hour) %>% 
    summarise(count = sum(count, na.rm = T)) %>% 
    ggplot(aes(x = hour, y = count, color = season)) +
    geom_line()

factor_list <- sapply(all_data, is.factor) %>%
  which()

lst <- lapply(factor_list, function(i) {
  df_list <- colnames(all_data)[i]

  all_data %>%
    rename(aa = df_list) %>%
    group_by(aa, hour) %>%
    summarise(count = sum(count, na.rm = T)) %>%
    ggplot(aes(x = hour, y = count, group = aa, colour = aa)) +
    labs(title = paste0("Count by ",df_list), x = "Hour",  color = df_list) +
    theme_bw() +
    geom_line(size = 1.5, alpha = 0.7)
})

grid.arrange(grobs=lst, ncol=2)

```

```{r}

bike_res <- all_data %>% 
    recipe(count~.) %>% 
    step_rm(datetime, year) %>% 
    # step_downsample()
    # update_role()
    # step_boxCox(all_numeric())
    # step_smote()
    # step_log(count) %>% 
    step_dummy(all_nominal()) %>% 
    step_nzv(all_numeric()) %>% 
    step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
    #step_normalize(all_numeric(), -all_outcomes()) %>% 
    prep(training = all_data)
    
```

```{r}
all_data2 <- juice(bike_res)
train_index <- seq_len(nrow(train))
train2 <- all_data2[train_index,]
test2 <- all_data2[-train_index,]
```


# XGboost 
```{r}
xgb_spec <- boost_tree(
    trees = 1000, 
    tree_depth = tune(),
    min_n = tune(), 
    loss_reduction = tune(), 
    sample_size = tune(),
    mtry = tune(), 
    learn_rate = tune()
) %>% 
    set_engine('xgboost', objective = "reg:squarederror") %>% 
    set_mode('regression')
```

```{r}
xgb_grid <- grid_latin_hypercube(
    tree_depth(), 
    min_n(), 
    loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), train2), 
    learn_rate(), 
    size = 30
)
xgb_grid
```



```{r}
xgb_wf <- workflow() %>% 
    add_formula(count~.) %>% 
    add_model(xgb_spec)

```

```{r}
set.seed(1234)
vb_folds <- vfold_cv(train2, strata = count)
vb_folds
```

```{r}
library(tictoc)
# tic()
doParallel::registerDoParallel()
set.seed(1234)

xgb_res <- tune_grid(
    xgb_wf, 
    resamples = vb_folds,
    grid = xgb_grid, 
    control = control_grid(save_pred = TRUE)
)
# toc() 1023.77
```

```{r}
show_best(xgb_res, 'rmse')
best_param <- select_best(xgb_res, 'rmse')

final_xgb <- finalize_workflow(xgb_wf, best_param)
final_xgb

```


```{r}
library(vip)
final_xgb %>% 
    fit(data = train2) %>% 
    pull_workflow_fit() %>% 
    vip(geom = 'point')

```

```{r}
final_model <- finalize_model(xgb_spec, best_param)
final_workflow <- xgb_wf %>% update_model(final_model)
xgb_fit <- fit(final_workflow, data = train2)


pred_xgb <- 
    predict(xgb_fit, test2) %>% 
    mutate(modelo = "XGBoost")

pred_xgb %>% filter(.pred < 0)
```


# LASSO


```{r}
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

lasso_wf <- workflow() %>% 
    add_formula(count~.) %>% 
    add_model(lasso_spec)

lambda_grid <- grid_regular(penalty(), levels = 50)

set.seed(1234)
vb_folds <- vfold_cv(train2, strata = count)


doParallel::registerDoParallel()

lasso_res <- tune_grid(
    lasso_wf, 
    resamples = vb_folds, 
    grid = lambda_grid, 
    control = control_grid(save_pred = TRUE), 
)
```

```{r}
lasso_res %>% 
    collect_metrics() %>% 
    ggplot(aes(penalty, mean, color = .metric)) + # .metric : rmse 
    geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), alpha = 0.5) + 
    geom_line(size = 1.5, show.legend = F) + 
    facet_wrap(~.metric, scales = 'free', nrow = 2) +
    scale_x_log10()
    
```

```{r}
show_best(lasso_res, 'rmse')
best_param_lasso <- select_best(lasso_res, 'rmse')
final_model <- finalize_model(lasso_spec, best_param_lasso)
final_workflow <- lasso_wf %>% update_model(final_model)
lasso_fit <- fit(final_workflow, data = train2)

pred_lasso <- 
    predict(lasso_fit, test2) %>% 
    mutate(modelo = "LASSO")

pred_lasso
```
# random forest 
```{r}
set.seed(1234)
validation_split <- validation_split(train2, prop = 0.7)

```
```{r}
rf_spec <- rand_forest(
    mtry = tune(), 
    min_n = tune(), 
    trees = tune()
) %>% 
    set_mode('regression') %>% 
    set_engine('ranger', importance = 'impurity_corrected')


rf_wf <- workflow() %>% 
    add_formula(count~.) %>% 
    add_model(rf_spec)


set.seed(1234)
vb_folds <- vfold_cv(train2, strata = count)


doParallel::registerDoParallel()

set.seed(1234)
rf_folds <- bootstraps(train2, strata = count)

rf_res <- tune_grid(
    rf_wf, 
    resamples = rf_folds,
    grid = 11, 
    control = control_grid(save_pred = TRUE), 
)

show_best(rf_res, 'rmse')
best_param_rf <- select_best(rf_res, 'rmse')
final_model_rf <- finalize_model(rf_spec, best_param_rf)
final_workflow_rf <- rf_wf %>% update_model(final_model_rf)
rf_fit <- fit(final_workflow_rf, data = train2)

pred_rf <- 
    predict(rf_fit, test2) %>% 
    mutate(modelo = "RF")

pred_las
```


```{r}

subfile <- read_csv("C:/Users/uos/Desktop/kaggle-study/data/bike sharing/sampleSubmission.csv")
subfile 

subfile$count <- exp(pred_lasso$.pred)

write.csv(subfile, row.names = FALSE,
          "C:/Users/uos/Desktop/kaggle-study/data/bike sharing/bh_xgb.csv") 

write.csv(subfile, row.names = FALSE,
          "C:/Users/uos/Desktop/kaggle-study/data/bike sharing/bh_lasso.csv") 

# xgb : 0.50121 
# LASSO : 1.04176 

```



# bayes tune 
```{r}
# 
# params <- parameters(xgb_spec) %>%
#     finalize(train2)
# 
# xgboost_wflow <- workflow() %>%
#     add_recipe(bike_res) %>%
#     add_model(xgb_spec)
# 
# 
# library(parallel)
# options(tidymodels.dark = TRUE)
# cl <- makePSOCKcluster(6)
# set.seed(1234)
# folds <- vfold_cv(train2, v = 5, strata = 'count')
# 
# tuned <- tune_bayes(
#     object = xgboost_wflow,
#     resamples = folds,
#     param_info = params,
#     iter = 30,
#     metrics =  metric_set(rmse, mape),
#     initial = 10,
#     control = control_bayes(
#         verbose = TRUE,
#         no_improve = 10,
#         seed = 123
#      )
#  )
# 
```





