---
title: "tidymodels bayes tuning"
description: |
  tidymodels bayes tunig 소개 
author:
  - name: dondon
    url: {}
date: 2021-07-05
output:
  distill::distill_article:
    number_sections: true 
    fig_caption: true
    toc: true
    #toc_depth: 3
    self_contained: false
    #highlight: default
    #highlight_downlit: true
    #code_folding: show
ditor_options: 
  chunk_output_type: console
categories:
  - Machine learning
  - tidymodels
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
```

# Preparations {.tabset .tabset-fade}

## Libraries

```{r load_lib, message=FALSE, warning=FALSE, results='hide'}
library(tidymodels)
library(tidyverse)
library(lubridate)
library(skimr)
library(magrittr)
library(data.table)
library(gridExtra)
library(themis)

theme_set(theme_bw())
```

## Data load

```{r}
file_path <- "../input/bike-sharing-demand/"
files <- list.files(file_path)
files
```

```{r, message=FALSE}
train <- read_csv(file.path(file_path, "train.csv"))
test <- read_csv(file.path(file_path, "test.csv"))
```

# Data overview (데이터 기본정보) {.tabset .tabset-fade}

## train data
```{r}
head(train)
skim(train)
```

## test data

```{r}
head(test)
skim(test)

test %>% 
  summarise(across(.fns = ~sum(is.na(.))/length(.)))

```

# data preprocessing {.tabset .tabset-fade}

## combine train, test 

```{r}
all_data <- bind_rows(train, test)
```

## Change variable type

```{r}
all_data$season <- factor(all_data$season, labels = c('winter', 'fall', 'summer', 'spring'))
all_data$weather <- as.factor(all_data$weather)
all_data$workingday <- as.factor(all_data$workingday)
all_data$holiday <- as.factor(all_data$holiday)
```

## create date variable 

```{r}
all_data %>% mutate(year = year(datetime), 
                    month = month(datetime),
                    wday = wday(datetime),
                    day = day(datetime), 
                    hour = hour(datetime)) %>% 
    select(year, month, wday, day, holiday, workingday, everything()) -> all_data
```

## convert wday, month 

```{r}
all_data$wday <- factor(all_data$wday, labels = c('Sun', 'Mon', 'Tue', 'Wed', 'Thur', 'Fri', 'Sat'))
all_data$month <- as.factor(all_data$month)
```

# univariate visualization {.tabset .tabset-fade}

## count(target variable) 

```{r, message=FALSE, warning=FALSE}
all_data %>% 
    ggplot(aes(x = count)) + 
    geom_histogram()
```

count 변수를 보면 0인 count가 많음

## atemp, temp 

```{r, warning=FALSE, message=FALSE}
p1 <- all_data %>% 
    ggplot(aes(x = atemp)) + 
    geom_histogram()
p2 <- all_data %>% 
    ggplot(aes(x = temp)) + 
    geom_histogram()

grid.arrange(grobs = list(p1, p2), col = 2)
```

temp, atemp 분포는 거의 비슷함

## casual, registered 

```{r}
all_data %>% 
  ggplot(aes(x = registered)) + 
  geom_histogram()

all_data %>% 
  ggplot(aes(x = casual)) + 
  geom_histogram()


var(all_data$registered, na.rm = T)
mean(all_data$registered, na.rm = T)

var(all_data$casual, na.rm = T)
mean(all_data$casual, na.rm = T)

```

train data에는 존재하지만 test 데이터에는 존재하지 않는 count variable이다. 두 변수의 분포를 보면 0의 비율이 매우 많고, 과대산포되어있는 것을 볼 수 있다.

## Visualization of correlations and distributions (holiday)

```{r}
all_data %>% 
    select(holiday, temp, humidity, windspeed, count) %>% 
    GGally::ggpairs(mapping = aes(color = holiday))
```

## Visualization of correlations and distributions (workingday)

```{r}
all_data %>% 
    select(workingday, temp, humidity, windspeed, count) %>% 
    GGally::ggpairs(mapping = aes(color = workingday))
```

# factor variable visualization {.tabset .tabset-fade}

## season vs count 

```{r}
all_data %>% 
    group_by(season, hour) %>% 
    summarise(count = sum(count, na.rm = T)) %>% 
    ggplot(aes(x = hour, y = count, color = season)) +
    geom_line(size = 1.5, alpha = 0.7)
```

## weather vs count 

```{r}
all_data %>% 
    group_by(weather, hour) %>% 
    summarise(count = sum(count, na.rm = T)) %>% 
    ggplot(aes(x = hour, y = count, color = weather)) +
    geom_line(size = 1.5, alpha = 0.7)
```

## wday vs count 

```{r}
all_data %>% 
    group_by(wday, hour) %>% 
    summarise(count = sum(count, na.rm = T)) %>% 
    ggplot(aes(x = hour, y = count, color = wday)) +
    geom_line(size = 1.5, alpha = 0.7)
```

## holiday vs count 

```{r}
all_data %>% 
    group_by(holiday, hour) %>% 
    summarise(count = sum(count, na.rm = T)) %>% 
    ggplot(aes(x = hour, y = count, color = holiday)) +
    geom_line(size = 1.5, alpha = 0.7)
```

## workingday vs count

```{r}
all_data %>% 
    group_by(workingday, hour) %>% 
    summarise(count = sum(count, na.rm = T)) %>% 
    ggplot(aes(x = hour, y = count, color = workingday)) +
    geom_line(size = 1.5, alpha = 0.7)
```

## month vs count 

```{r}
all_data %>% 
    group_by(month, hour) %>% 
    summarise(count = sum(count, na.rm = T)) %>% 
    ggplot(aes(x = hour, y = count, color = month)) +
    geom_line(size = 1.5, alpha = 0.7)
```

## temp vs atemp 

```{r}
a1 <- all_data %>% 
  mutate(hour = as.factor(hour)) %>% 
  ggplot(aes(x=hour, y = registered)) + geom_boxplot()

a2 <- all_data %>% 
  mutate(hour = as.factor(hour)) %>% 
  ggplot(aes(x=hour, y = casual)) + geom_boxplot()

grid.arrange(a1, a2)
```

# Discretize 

humidity의 경우 0 값이 22개 존재함. humidiy 값이 0인 경우를 결측치 대체함. windspeed가 0인 경우도 마찬가지로 결측치로 보고 대체함 weather의 경우 4 level의 빈도가 매우 작기 때문에 3 level과 통합

```{r}
all_data %>% 
    recipe(count~.) %>% 
    step_other(weather, threshold = 0.1, other = 3) %>% 
    step_discretize(windspeed, min_unique = 5, num_breaks = 5) %>%
    #step_discretize_xgb(windspeed)
    prep(training = all_data) %>% 
    bake(new_data = all_data) -> all_data

table(all_data$windspeed)
```

# Recipe + preperation {.tabset .tabset-fade}

```{r}
bike_res <- all_data %>% 
  recipe(count~.) %>% 
  step_rm(datetime, registered, casual) %>%
  step_mutate(year = as.factor(year)) %>% 
  step_log(count, offset = 1) %>%
  step_dummy(all_nominal()) %>%
  step_nzv(all_numeric()) %>% 
  prep(training = all_data)
    
```

# Juice {.tabset .tabset-fade}

```{r}
all_data2 <- juice(bike_res)
```

# Split train, test {.tabset .tabset-fade}

```{r}
train_index <- seq_len(nrow(train))
train2 <- all_data2[train_index,]
test2 <- all_data2[-train_index,]
```

# XGboost setting {.tabset .tabset-fade}

## XGBOOST hyperparameter setting

```{r}
xgb_spec <- boost_tree(
    trees = 1000, # 앙상블에 포함되는 tree의 수 
    tree_depth = tune(), # 얼마만큼 노드를 split할건지 
    min_n = tune(), # 노드를 분할하는데 필요한 최소 데이터의 수
    loss_reduction = tune(), # 노드 분할에 필요한 loss의 감소량 
    sample_size = tune(), # The amount of data exposed to the fitting routine
    mtry = tune(), # The number of predictors that will be randomly sampled at each split when creating the tree models. 
    learn_rate = tune() 
) %>% 
    set_engine('xgboost', objective = "reg:squarederror") %>% 
    set_mode('regression')

params <- parameters(xgb_spec) %>% 
    finalize(train2)
params
```

# XGboost workflow {.tabset .tabset-fade}

## workflow model setting

```{r}
xgb_wf <- workflow() %>% 
    add_formula(count~.) %>% 
    add_model(xgb_spec)
```

## cross validation

```{r}
set.seed(2021)
vb_folds <- vfold_cv(train2, v = 5, strata = count)
vb_folds
```

## hyperparameter tuning

```{r}
library(tictoc)
tic()
doParallel::registerDoParallel()
xgb_res <- tune_bayes(
    object = xgb_wf, # recipe, formula를 지정해준 workflow 
    resamples = vb_folds, 
    param_info = params, 
    iter = 30, 
    metrics = metric_set(rmse), 
    initial = 10, 
    control = control_bayes(
        verbose = TRUE, 
        no_improve = 10, 
        seed = 123) 
)
toc()  
```

## Final model update

```{r}
best_param <- select_best(xgb_res, 'rmse')
final_xgb <- finalize_workflow(xgb_wf, best_param)
final_xgb
```

## final model setting

```{r}
final_model <- finalize_model(xgb_spec, best_param) 
final_model # tuning이 끝난 최종 모형 
```

## final model workflow에 업데이트

```{r}
final_workflow <- xgb_wf %>% update_model(final_model)
```

## final model 학습

```{r}
xgb_fit <- fit(final_workflow, data = train2)
```

# Result {.tabset .tabset-fade}

## Prediction 

```{r}
pred_xgb <- 
    predict(xgb_fit, test2) %>% 
    mutate(modelo = "XGBoost")
pred_xgb %>% head()

pred_xgb$.pred <- exp(pred_xgb$.pred)-1
```

## feature importance plot

```{r}
library(vip) # feature importance plot 그리기 
final_xgb %>% 
    fit(data = train2) %>%  # iter, training_rmse 
    pull_workflow_fit() %>% #  http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/
    vip(geom = 'point')
```

# Submit file {.tabset .tabset-fade}

```{r}
subfile <- read_csv(file.path(file_path, "sampleSubmission.csv"))
subfile$count <- pred_xgb$.pred

write.csv(subfile, row.names = FALSE,
          file.path(file_path, "xgb_Submission_bayestuning_discretize.csv")) 
```
