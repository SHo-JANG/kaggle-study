---
title: "Untitled"
output: html_document
---


* datetime - hourly date + timestamp  
* season -  1 = spring, 2 = summer, 3 = fall, 4 = winter 
* holiday - whether the day is considered a holiday
* workingday - whether the day is neither a weekend nor holiday
* weather 
- 1: Clear, Few clouds, Partly cloudy, Partly cloudy
- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog 
* temp - temperature in Celsius
* atemp - "feels like" temperature in Celsius ( ‘feels like temperature by taking into account the expected air temperature, relative humidity and the strength of the wind at around 5 feet (the typical height of an human face))
* humidity - relative humidity
* windspeed - wind speed
* casual - number of non-registered user rentals initiated
* registered - number of registered user rentals initiated
* count - number of total rentals



```{r, include=F, message=F}
library(tidymodels)
library(tidyverse)
library(lubridate)
library(skimr)
library(magrittr)
library(data.table)
library(gridExtra)

theme_set(theme_bw())
```


```{r}
train <- read_csv("C:/Users/sangdon/Desktop/kaggle-study/data/bike sharing/train.csv")
test <- read_csv("C:/Users/sangdon/Desktop/kaggle-study/data/bike sharing/test.csv")
```

```{r}
head(train)
names(train)
names(test)
```

```{r}
train %>% 
    select(-c(casual, registered)) -> train
```


```{r}
all_data <- bind_rows(train, test)

glimpse(all_data)
all_data$holiday %>% unique()
all_data$workingday %>% unique()
all_data$weather %>% unique()

all_data$season <- factor(all_data$season, labels = c('spring', 'summer', 'fall', 'winter'))
all_data$weather <- as.factor(all_data$weather)
all_data$workingday <- as.factor(all_data$workingday)
all_data$holiday <- as.factor(all_data$holiday)

all_data %>% mutate(year = year(datetime), 
                    month = month(datetime),
                    wday = wday(datetime),
                    day = day(datetime), 
                    hour = hour(datetime)) %>% 
    select(year, month, wday, day, holiday, workingday, everything()) -> all_data

all_data$wday <- factor(all_data$wday, labels = c('Sun', 'Mon', 'Tue', 'Wed', 'Thur', 'Fri', 'Sat'))
all_data$month <- as.factor(all_data$month)
all_data
```


```{r}
skim(all_data)

```

```{r}

all_data %>% 
    ggplot(aes(x = count)) + 
    geom_histogram()
p1 <- all_data %>% 
    ggplot(aes(x = atemp)) + 
    geom_histogram()
p2 <- all_data %>% 
    ggplot(aes(x = temp)) + 
    geom_histogram()

grid.arrange(grobs = list(p1, p2), col = 2)

all_data %>% 
    select(holiday, temp, humidity, windspeed, count) %>% 
    GGally::ggpairs(mapping = aes(color = holiday))

all_data %>% 
    select(workingday, temp, humidity, windspeed, count) %>% 
    GGally::ggpairs(mapping = aes(color = workingday))
```
count 변수를 보면 0인 count가 많음 
temp, atemp 분포 거의 비슷


```{r}

all_data %>% 
    group_by(season, hour) %>% 
    summarise(count = sum(count, na.rm = T)) %>% 
    ggplot(aes(x = hour, y = count, color = season)) +
    geom_line()

factor_list <- sapply(all_data, is.factor) %>% 
  which()

lst <- lapply(factor_list, function(i) {
  df_list <- colnames(all_data)[i]

  all_data %>%
    rename(aa = df_list) %>%
    group_by(aa, hour) %>%
    summarise(count = sum(count, na.rm = T)) %>%
    ggplot(aes(x = hour, y = count, group = aa, colour = aa)) +
    labs(title = paste0("Count by ",df_list), x = "Hour",  color = df_list) +
    theme_bw() +
    geom_line(size = 1.5, alpha = 0.7)
})

grid.arrange(grobs=lst, ncol=2)

```

```{r}

bike_res <- all_data %>% 
    recipe(count~.) %>% 
    step_rm(datetime, year) %>% 
    # step_downsample() : category 변수의 level에서 빈도가 가장 낮은 level로 변환 
    # step_boxCox(all_numeric()) : boxcox transformation : 분포의 정규성 맞춰줌  
    # step_smote() : category level 빈도가 불균형일 때 
    step_log(count, offset = 1) %>% 
    step_dummy(all_nominal()) %>% 
    step_nzv(all_numeric()) %>% 
    #step_YeoJohnson(all_numeric(), -all_outcomes()) %>% : boxcox와 비슷 
    #step_normalize(all_numeric(), -all_outcomes()) %>% 
    prep(training = all_data)
    
```

```{r}
all_data2 <- juice(bike_res)
train_index <- seq_len(nrow(train))
train2 <- all_data2[train_index,]
test2 <- all_data2[-train_index,]
```


# XGboost 
```{r}
xgb_spec <- boost_tree(
    trees = 1000, # 앙상블에 포함되는 tree의 수 
    tree_depth = tune(), # 얼만큼 노드를 split할건지 
    min_n = tune(), # 노드를 분할하는데 필요한 최소 데이터의 수
    loss_reduction = tune(), # 노드 분할에 필요한 loss의 감소량 
    sample_size = tune(), # The amount of data exposed to the fitting routine
    mtry = tune(), # The number of predictors that will be randomly sampled at each split when creating the tree models. 
    learn_rate = tune() 
) %>% 
    set_engine('xgboost', objective = "reg:squarederror") %>% 
    set_mode('regression')
```

```{r}
xgb_grid <- grid_latin_hypercube(
    tree_depth(), 
    min_n(), 
    loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), train2), # mtry() : [1, ?], finalize(mtry(), train2) : [1, 30]
    learn_rate(), 
    size = 30
)
```



```{r}
xgb_wf <- workflow() %>% 
    add_formula(count~.) %>% 
    add_model(xgb_spec)

```

```{r}
set.seed(1234)
vb_folds <- vfold_cv(train2, v = 5, strata = count)
vb_folds
```

```{r}
library(tictoc)
tic()
doParallel::registerDoParallel()
set.seed(1234)

xgb_res <- tune_grid(
    xgb_wf, # recipe, formula를 지정해준 workflow 
    resamples = vb_folds, 
    grid = xgb_grid, # grid_latin_hypercube
    control = control_grid(save_pred = TRUE) # out of sample predicton 값 저장. test data에 fitting 한 뒤에 collect_prediction()으로 예측값을 확인할 때 사용됨. classification 문제에서는 roc curve를 그리는데에도 활용됨   
)
toc() # 540.33 sec 
```

```{r}
show_best(xgb_res, 'rmse')
best_param <- select_best(xgb_res, 'rmse')

final_xgb <- finalize_workflow(xgb_wf, best_param)
final_xgb

```


```{r}
library(vip) # feature importance plot 그리기 
final_xgb %>% 
    fit(data = train2) %>%  # iter, training_rmse 
    pull_workflow_fit() %>% #  http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/
    vip(geom = 'point')

```

```{r}
final_model <- finalize_model(xgb_spec, best_param) 
final_model # tuning이 끝난 최종 모형 

final_workflow <- xgb_wf %>% update_model(final_model)
xgb_fit <- fit(final_workflow, data = train2)


pred_xgb <- 
    predict(xgb_fit, test2) %>% 
    mutate(modelo = "XGBoost")

pred_xgb %>% filter(.pred < 0)
```


# LASSO 

```{r}
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>% # mixture = 1 : LASSO, 0 : ridge 
  set_engine("glmnet")

lasso_wf <- workflow() %>% 
    add_formula(count~.) %>% 
    add_model(lasso_spec)

lambda_grid <- grid_regular(penalty(), levels = 50) # https://cran.r-project.org/web/packages/dials/vignettes/Basics.html

tic()
doParallel::registerDoParallel()
lasso_res <- tune_grid(
    lasso_wf, 
    resamples = vb_folds, 
    grid = lambda_grid, 
    control = control_grid(save_pred = TRUE), 
)
toc() # 12.14 sec
```


```{r}
lasso_res %>% 
    collect_metrics()

lasso_res %>% 
    collect_metrics() %>% 
    ggplot(aes(penalty, mean, color = .metric)) + # .metric : rmse 
    geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), alpha = 0.5) + 
    geom_line(size = 1.5, show.legend = F) + 
    facet_wrap(~.metric, scales = 'free', nrow = 2) 
    
```

```{r}
show_best(lasso_res, 'rmse')
best_param_lasso <- select_best(lasso_res, 'rmse')
final_model <- finalize_model(lasso_spec, best_param_lasso)
final_workflow <- lasso_wf %>% update_model(final_model)
lasso_fit <- fit(final_workflow, data = train2)

pred_lasso <- 
    predict(lasso_fit, test2) %>% 
    mutate(modelo = "LASSO")

pred_lasso
```
# random forest 
#```{r}
#set.seed(1234)
#validation_split <- validation_split(train2, prop = 0.7)

#```

#```{r}
#rf_spec <- rand_forest(
#    mtry = tune(), 
#    min_n = tune(), 
#    trees = tune()
#) %>% 
#    set_mode('regression') %>% 
#    set_engine('ranger', importance = 'impurity_corrected')

#rf_wf <- workflow() %>% 
#    add_formula(count~.) %>% 
#    add_model(rf_spec)


#tic()
#doParallel::registerDoParallel()

#set.seed(1234)
#rf_folds <- bootstraps(train2, strata = count)

#rf_res <- tune_grid(
#    rf_wf, 
#    resamples = rf_folds,
#    grid = 11, 
#    control = control_grid(save_pred = TRUE), 
#)
#toc()


#show_best(rf_res, 'rmse')
#best_param_rf <- select_best(rf_res, 'rmse')
#final_model_rf <- finalize_model(rf_spec, best_param_rf)
#final_workflow_rf <- rf_wf %>% update_model(final_model_rf)
#rf_fit <- fit(final_workflow_rf, data = train2)

#pred_rf <- 
#    predict(rf_fit, test2) %>% 
#    mutate(modelo = "RF")

#```


```{r}

#subfile <- read_csv("C:/Users/uos/Desktop/kaggle-study/data/bike #sharing/sampleSubmission.csv")
#subfile 

#subfile$count <- exp(pred_lasso$.pred)

#write.csv(subfile, row.names = FALSE,
#          "C:/Users/uos/Desktop/kaggle-study/data/bike #sharing/bh_xgb.csv") 

#write.csv(subfile, row.names = FALSE,
#          "C:/Users/uos/Desktop/kaggle-study/data/bike #sharing/bh_lasso.csv") 

# xgb : 0.50121 
# LASSO : 1.04176 

```



# bayes tune 
```{r}
# 
# params <- parameters(xgb_spec) %>%
#     finalize(train2)
# 
# xgboost_wflow <- workflow() %>%
#     add_recipe(bike_res) %>%
#     add_model(xgb_spec)
# 
# 
# library(parallel)
# options(tidymodels.dark = TRUE)
# cl <- makePSOCKcluster(6)
# set.seed(1234)
# folds <- vfold_cv(train2, v = 5, strata = 'count')
# 
# tuned <- tune_bayes(
#     object = xgboost_wflow,
#     resamples = folds,
#     param_info = params,
#     iter = 30,
#     metrics =  metric_set(rmse, mape),
#     initial = 10,
#     control = control_bayes(
#         verbose = TRUE,
#         no_improve = 10,
#         seed = 123
#      )
#  )
# 
```




