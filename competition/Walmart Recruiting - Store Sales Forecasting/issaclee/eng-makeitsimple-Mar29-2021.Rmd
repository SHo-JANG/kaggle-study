---
title: "Make it simple."
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 5
    fig_height: 4
    theme: cosmo
    highlight: tango
    code_folding: show
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.align = "center")
```


![Photo steal from [here](https://connectedremag.com/das-in-building-wireless/walmart-verizon-explore-testing-5g-in-some-stores/)](https://connectedremag.com/wp-content/uploads/2020/03/walmart-5G-connected-real-estate.png)

This posting is the part of my youtube channel study about [Kaggling with R](https://www.youtube.com/playlist?list=PLKtLBdGREmMlJCXjCpCi5B4KQ-TsFvAAi).

# Back to the basic.

I was playing around this old competion for a while. I have tried to use regression based model (Elastic net) and tree model (Random forest) using tidymodel. If you see the dataset, this is basically year, month, store, and department based sales price. Therefore, the tree based model performs better in this dataset by its nature. However, I was surprised to see that [the discussion](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/discussion/8033) from [David Thaler](https://www.kaggle.com/davidthaler), who took thefirst place of this competition and now he is the Grandmaster, saying that the group mean by month, store, dept, and is_holyday can beat regression and random forest easily. This notebook will show that the simple group mean can beat most of the competitors. 

# Preperation {.tabset .tabset-fade}

## Library load

Load R pacakges for Kaggling. I usually use these for any notebook thesedays, why not in here for calculating group mean? ðŸ¤£

```{r load_lib, message=FALSE, warning=FALSE, results='hide'}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(skimr)
library(knitr)
theme_set(theme_bw())
```

## Dataset load

These are the file list in the competition. We will only use the `train` and `test` data for this notebook. Remember, make it simple.

```{r}
file_path <- "../input/walmart-recruiting-store-sales-forecasting/"
files <- list.files(file_path)
files
```
One note; I love to convert the variables names by using `janitor` package because the result are consist and not distracting.

```{r, message=FALSE}
train <- read_csv(file.path(file_path, "train.csv.zip")) %>% 
  janitor::clean_names()
test <- read_csv(file.path(file_path, "test.csv.zip")) %>% 
  janitor::clean_names()
```

# Basic info. {.tabset .tabset-fade}

## Basic info.

This competition data set has very simple structure; that was the reason why I was digging into this as my study material.

`train` has 420,000 samples and `train` has 110,000 samples.

```{r}
dim(train)
dim(test)
```

The list of the variable we have are as follows:

* `store`: walmart store
* `dept`: department in a given store
* `date`: date is date.
* `is_holiday`: holiday indicator

We can see that the only `train` set has `weekly_sales` variable which is our target variable.

```{r}
names(train)
names(test)
```

## train and test data snippet

```{r}
head(train) %>% kable()
head(train) %>% kable()
```

# weekly_sales visualization {.tabset .tabset-fade}

The distribution of the `weekly_sales` is highly skewed to the right.

```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
  ggplot(aes(x = weekly_sales)) +
  geom_histogram()
```

If we take `log` with offset 2, the skewness will be corrected little bit.

```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
    ggplot(aes(x = sign(weekly_sales) * log(abs(weekly_sales) + 2))) +
    geom_histogram() +
    labs(title = "Transformed distribution of weekly sales 1",
         x = "weekly_sales")
```

Power of 1/5 will make the distribution to be really similar to symetrical distribution.

```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
    ggplot(aes(x = sign(weekly_sales) * (abs(weekly_sales))^(1/5))) +
    geom_histogram() +
    labs(title = "Transformed distribution of weekly sales 2",
         x = "weekly_sales")
```

# Preprocessing with `recipe`

## `all_data` combine

Before I combine the `train` and `test` set, I transformed `weekly_sale`, and then combined them into `all_data`.

```{r}
train$weekly_sales <- sign(train$weekly_sales) * (abs(train$weekly_sales))^(1/5)
all_data <- bind_rows(train, test)
all_data %>% head()
names(all_data)
dim(all_data)
```

## Make recipe

I used recipe because I want to keep my notebook structure even if we use mean model. The following recipe is just simple like this;

* pull `month` from `date` variable
* remove the `date` variable

Done.

```{r}
walmart_recipe <- all_data %>% 
    recipe(weekly_sales ~ .) %>%
    step_mutate(month = lubridate::month(date)) %>% 
    step_rm(date) %>% 
    prep(training = all_data)

all_data2 <- juice(walmart_recipe)
all_data2 %>% dim()
all_data2 %>% head() %>% 
  kable()
```

`juice()` function will give you the actual preprocessed data.

# Calculate mean model

## Split the data set

Let's split the data again since we are done for the preprocessing.

```{r}
train_index <- seq_len(nrow(train))
train2 <- all_data2[train_index,]
test2 <- all_data2[-train_index,]
```

## store-dept-month, and holiday

Using `group_by` from tidyverse, we can simply calculate the group means for the four varibles and left join to the test data! Also note that I transformed back to the normal scale.

```{r}
median_model <- train2 %>% 
  group_by(store, dept, month, is_holiday) %>% 
  summarise(weekly_sales = median(weekly_sales, rm.na = TRUE))

result <- test2 %>% 
  select(-weekly_sales) %>% 
  left_join(y = median_model, 
            by = c("store"="store",
                   "dept" ="dept",
                   "month"="month",
                   "is_holiday"="is_holiday")) %>% 
  select(weekly_sales) %>% 
  mutate(weekly_sales = sign(weekly_sales) * (abs(weekly_sales)^5)) %>% 
  unlist() %>% as.numeric()

# result2 <- all_data2 %>% 
#   select(-weekly_sales) %>% 
#   left_join(y = median_model, 
#             by = c("store"="store",
#                    "dept" ="dept",
#                    "month"="month",
#                    "is_holiday"="is_holiday")) %>% 
#   select(weekly_sales) %>% 
#   mutate(weekly_sales = sign(weekly_sales) * (abs(weekly_sales)^5)) %>% 
#   rename(group_mean = weekly_sales)
result[is.na(result)] <- 0
```

After the left join, I substitute the NA's into 0's.

```{r}
# Manage NA's
result %>% head()
is.na(result) %>% sum()
result[is.na(result)] <- 0
```

Well, we are done! Let's submit our result!

# Submission

```{r}
submission <- read_csv(file.path(file_path, "sampleSubmission.csv.zip"))
submission$Weekly_Sales <- result
write.csv(submission, row.names = FALSE,
          "mean_model.csv")
submission %>% head()

# write.csv(result2, row.names = FALSE,
#           "group_mean.csv")
```
