---
title: "Separate Models to Predict Total Bike Rentals"
date: '`r Sys.Date()`'
output: 
  html_document:
    toc: true
    toc_float: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r, message = FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(lubridate)
library(skimr)
library(magrittr)
library(corrr)
library(vip)
library(testthat)
library(parallel)
library(doParallel)
```


## Understanding the data

```{r}
file_path <- "../input/bike-sharing-demand"
files <- list.files(file_path)
files
```

```{r}
# 2011-2012 bikeshare system data with weather/season/user information
train <- read_csv(file.path(file_path, "train.csv")) # 1-19th
test <- read_csv(file.path(file_path, "test.csv")) # predict rests 9-12 days
c("registered", "causal", "count") %in% names(test)
```


```{r}
all_data <- bind_rows(train, test)

tapply(all_data$temp, all_data$season, mean)
tapply(all_data$temp, all_data$weather, mean)

table(all_data$weather)
table(test$weather)
table(train$weather)

all_data <- all_data %>%
   mutate(year = year(datetime),
          month = month(datetime),
          wday = wday(datetime),
          day = day(datetime), 
          hour = hour(datetime),
          across(c(workingday, holiday, month, weather, season, wday), as.factor),
          season = factor(season, labels = c('winter', 'spring', 'summer', 'fall')),
          wday = factor(wday, labels = c('Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday')),
          weather = factor(weather, labels = c('clear', 'cloudy', 'bad', 'worse')),
          weather = fct_collapse(weather,rain = c("bad", "worse")))
skim(all_data)
```


```{r}
train_index <- seq_len(nrow(train))
train2 <- all_data[train_index,]
```


```{r}
# check correlations
corrDF <- train2 %>%
    select(-c(datetime, season, holiday, workingday, weather, month, wday)) %>% 
    correlate() %>%
    rearrange() %>% 
    shave()
rplot(corrDF)

#temperature, feels-like temperature: positive
#humidity: negative
```

```{r}
train2 %>%
    group_by(month) %>%
    summarise(registeredSum = sum(registered),
              casualSum = sum(casual)) %>%
    ggplot(aes(x = month, y = registeredSum)) +
    theme_bw() +
    geom_point() + 
    geom_point(aes(x = month, y = casualSum, colour="red"), show.legend=F)
```

```{r}
train2 %>%
    group_by(hour) %>%
    summarise(registeredSum = sum(registered),
              casualSum = sum(casual)) %>%
    ggplot(aes(x = hour, y = registeredSum)) +
    theme_bw() +
    geom_point() + 
    geom_point(aes(x = hour, y = casualSum, colour="red"), show.legend=F)


expect_equal(train$casual+train$registered, train$count)
```


The pattern of bike rents are different for registered and non-registered users. Perhaps, the bike rental counts are modeled separately for registered users and non-registered users. Today, I will fit a separate model for two types of users for parameter tuning, and then combine bike rental counts from registered and causal users to get total bike rentals. Benchmark is last week's model.


## Benchmark Model (XGboost)

```{r}
# replication of last week's model
copy <- all_data
all_data <- all_data %>% 
  select(-c(casual, registered))

# set recipe
bike_res <- all_data %>% 
    recipe(count~.) %>% 
    step_rm(datetime, year) %>% 
    step_log(count, offset = 1) %>% 
    step_dummy(all_nominal()) %>% 
    step_nzv(all_numeric()) %>% 
    prep(training = all_data)

# split data
all_data2 <- juice(bike_res)
train2 <- all_data2[train_index,]
test2 <- all_data2[-train_index,]

# set model: 
xgb_spec <- boost_tree(
    trees = 1000,
    # 6
    tree_depth = tune(), min_n = tune(), loss_reduction = tune(),
    sample_size = tune(), mtry = tune(), learn_rate = tune() 
) %>% 
    set_engine('xgboost', objective = "reg:squarederror") %>% 
    set_mode('regression')

# set grid
# grid_latin_hypercube: try to cover the parameter space; alternatively you can use "grid_regular"
xgb_grid <- grid_latin_hypercube(
    tree_depth(), 
    min_n(), 
    loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), train2),
    learn_rate(), 
    size = 30
)
xgb_grid

# set workflow
xgb_wf <- workflow() %>% 
    add_formula(count~.) %>% 
    add_model(xgb_spec)
```

```{r, eval=F}
# tuning
set.seed(1234)
vb_folds <- vfold_cv(train2, v = 5, strata = count)
vb_folds

doParallel::registerDoParallel()

xgb_res <- tune_grid(
    xgb_wf,
    resamples = vb_folds, 
    grid = xgb_grid,
    control = control_grid(save_pred = TRUE)  
)
```


```{r}
# visualize results
xgb_res %>% 
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry: sample_size,
               names_to="parameter",
               values_to="value") %>%
  ggplot(aes(value, mean, color=parameter)) +
  geom_point(show.legend=F) +
  facet_wrap(~parameter, scales="free_x")
```

```{r}
# select the best one
show_best(xgb_res, 'rmse')
best_param <- select_best(xgb_res, 'rmse')
best_param
```

```{r}
# finalize workflow
final_xgb <- finalize_workflow(xgb_wf, best_param)
final_xgb
```

```{r}
# variable importance plot
final_xgb %>% 
    fit(data = train2) %>%
    pull_workflow_fit() %>%
    vip(geom = 'point')
# hour (dominant), atemp, humidity, temp, workingday, day, windspeed
```

```{r}
# update the workflow with the final model
final_model <- finalize_model(xgb_spec, best_param) 
final_workflow <- xgb_wf %>% update_model(final_model)
xgb_fit <- fit(final_workflow, data = train2)
```

```{r}
# make a prediction
pred_xgb <- 
    predict(xgb_fit, test2) %>% 
    mutate(modelo = "XGBoost",
           .predexp = exp(.pred))

# we should have positive predictions
expect_gt(min(pred_xgb$.predexp), -0.001, "warning: we have negative counts")
```

```{r}
# submission
subfile <- read_csv(file.path(file_path, "sampleSubmission.csv"))
subfile$count <- pred_xgb$.predexp
write.csv(subfile, row.names = FALSE,
          "../minhee/xgboost_benchmark.csv")

# Last week xgb : 0.50121 
# this week xgb (collapse weather factors) : 0.47274 
```


## Separate Predictions (XGboost)

```{r}
all_data <- copy

# set recipe
registered_recipe <- recipe(registered ~., data=all_data) %>%
    step_rm(datetime, year, count, casual) %>% 
    step_log(registered, offset = 1) %>% 
    step_dummy(all_nominal()) %>% 
    step_nzv(all_numeric()) %>% 
    prep(training = all_data)

# split data
registered_all_data <- juice(registered_recipe)
registered_train <- registered_all_data[train_index,]
registered_test <- registered_all_data[-train_index,]

# set grid
xgb_grid_registered <- grid_latin_hypercube(
    tree_depth(), 
    min_n(), 
    loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), registered_train),
    learn_rate(), 
    size = 30
)

# set workflow
xgb_wf_registered <- workflow() %>% 
    add_formula(registered~.) %>% 
    add_model(xgb_spec)
```

```{r}
# tuning
set.seed(1234)
vb_folds_registered <- vfold_cv(registered_train, v = 5, strata = registered)
vb_folds_registered

doParallel::registerDoParallel()

xgb_res_registered <- tune_grid(
    xgb_wf_registered,
    resamples = vb_folds_registered, 
    grid = xgb_grid_registered,
    control = control_grid(save_pred = TRUE)  
)
```

```{r}
# visualize results
xgb_res_registered %>% 
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry: sample_size,
               names_to="parameter",
               values_to="value") %>%
  ggplot(aes(value, mean, color=parameter)) +
  geom_point(show.legend=F) +
  facet_wrap(~parameter, scales="free_x")
```

```{r}
# select the best one
show_best(xgb_res_registered, 'rmse')
best_param_registered <- select_best(xgb_res_registered, 'rmse')

# finalize workflow
final_xgb_registered <- finalize_workflow(xgb_wf_registered, best_param_registered)

```

```{r}
# draw variable importance plot
final_xgb_registered %>% 
    fit(data = registered_train) %>% 
    pull_workflow_fit() %>%
    vip(geom = 'point')
# hour (dominant), workingday, temp, humidity, atemp
```

```{r}
# update the workflow with the final model
final_model_registered <- finalize_model(xgb_spec, best_param_registered) 
final_workflow_registered <- xgb_wf_registered %>% update_model(final_model_registered)
xgb_fit_registered <- fit(final_workflow_registered, data = registered_train)
```

```{r}
# make a prediction
pred_xgb_registered <- 
    predict(xgb_fit_registered, registered_test) %>% 
    mutate(modelo = "XGBoost",
           .predexp = exp(.pred))

# we should not have negative predictions
expect_gt(min(pred_xgb_registered$.predexp), -0.001, "we have negative counts")
```

```{r}
# same procedure for causal model
casual_recipe <- recipe(casual ~., data=all_data) %>%
    step_rm(datetime, year, count, registered) %>% 
    step_log(casual, offset = 1) %>% 
    step_dummy(all_nominal()) %>% 
    step_nzv(all_numeric()) %>% 
    prep(training = all_data)

# split data
casual_all_data <- juice(casual_recipe)
casual_train <- casual_all_data[train_index,]
casual_test <- casual_all_data[-train_index,]

# set grid
xgb_grid_casual <- grid_latin_hypercube(
    tree_depth(), 
    min_n(), 
    loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), casual_train),
    learn_rate(), 
    size = 30
)

# set workflow
xgb_wf_casual <- workflow() %>% 
    add_formula(casual~.) %>% 
    add_model(xgb_spec)
```



```{r}
set.seed(1234)
vb_folds_casual <- vfold_cv(casual_train, v = 5, strata = casual)
vb_folds_casual

doParallel::registerDoParallel()

# tuning
xgb_res_casual <- tune_grid(
    xgb_wf_casual, 
    resamples = vb_folds_casual, 
    grid = xgb_grid_casual, 
    control = control_grid(save_pred = TRUE)   
)
```

```{r}
# visualize results
xgb_res_casual %>% 
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry: sample_size,
               names_to="parameter",
               values_to="value") %>%
  ggplot(aes(value, mean, color=parameter)) +
  geom_point(show.legend=F) +
  facet_wrap(~parameter, scales="free_x")
```

```{r}
# select the best model
show_best(xgb_res_casual, 'rmse')
best_param_casual <- select_best(xgb_res_casual, 'rmse')

# finalize workflow
final_xgb_casual <- finalize_workflow(xgb_wf_casual, best_param_casual)
```

```{r}
# draw variable importance plot
final_xgb_casual %>% 
    fit(data = casual_train) %>%
    pull_workflow_fit() %>%
    vip(geom = 'point')
# hour (dominant), temp, atemp, humidity, workingday, day
```

```{r}
# update the workflow with the final model
final_model_casual <- finalize_model(xgb_spec, best_param_casual) 
final_workflow_casual <- xgb_wf_casual %>% update_model(final_model_casual)
xgb_fit_casual <- fit(final_workflow_casual, data = casual_train)
```

```{r}
# make a prediction
pred_xgb_casual <- 
    predict(xgb_fit_casual, casual_test) %>% 
    mutate(modelo = "XGBoost",
           .predexp = exp(.pred))

# we should not have negative predictions
expect_gt(min(pred_xgb_casual$.predexp), -0.001, "we have negative counts")

```

```{r}
subfile <- read_csv(file.path(file_path, "sampleSubmission.csv"))
subfile$count <- pred_xgb_casual$.predexp + pred_xgb_registered$.predexp

write.csv(subfile, row.names = FALSE,
          "../minhee/xgboost_separate.csv")

# Last week xgboost : 0.50121 
# this week xgboost : 0.47274 
# separate models : 0.44918
```



## Changes in Total Count Model

```{r}
copy <- all_data

hist(log(train$count+1))
hist(log(train$registered+1))
hist(log(train$casual+1))
hist((train$count)^(1/4))

# rmsle

# adding steps
# 1. temp
cor(train$temp, train$atemp)
train$new.temp <- (train$temp + train$atemp)/2
cor(train$temp, train$new.temp)
# 2. lagged outcome, lagged hours
# 3. scale weater-related vars
# 4. use 1/4, not log 
#all_data$new.count <- (all_data$count)^(1/4)


# set recipe
bike_res_new <- all_data %>% 
    recipe(count~.) %>% 
    #step_lag(count, lag=1) %>% # lag count
    #step_corr(all_numeric(), threshold = .9)
    #step_lag(hour, lag=1:2) %>% # lag hour
    step_mutate(
    new.temp = (temp + atemp)/2) %>%
    step_rm(datetime, year, temp, atemp,casual, registered) %>%  # remove atemp, temp, and include new.temp
    step_scale(windspeed, humidity, new.temp) %>% # scale
    step_log(count, offset = 1) %>% 
    step_dummy(all_nominal(),-all_outcomes()) %>%
    step_nzv(all_numeric(), -all_outcomes()) %>%
    prep(training = all_data)


# split data
all_data3 <- juice(bike_res_new)
train.new <- all_data3[train_index,]
test.new <- all_data3[-train_index,]

# set grid
xgb_grid.new <- grid_latin_hypercube(
    tree_depth(), min_n(), loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), train.new),
    learn_rate(), 
    size = 30
)

# set workflow
xgb_wf.new <- workflow() %>% 
    add_formula(count~.) %>% 
    add_model(xgb_spec)
```

```{r}
set.seed(1234)
vb_folds_new <- vfold_cv(train.new, v = 5, strata = count)
vb_folds_new

# tuning
doParallel::registerDoParallel()

xgb_res.new <- tune_grid(
    xgb_wf.new,
    resamples = vb_folds_new, 
    grid = xgb_grid.new,
    control = control_grid(save_pred = TRUE)  
)
```

```{r}
# visualize results
xgb_res.new %>% 
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry: sample_size,
               names_to="parameter",
               values_to="value") %>%
  ggplot(aes(value, mean, color=parameter)) +
  geom_point(show.legend=FALSE) +
  facet_wrap(~parameter, scales="free_x")
```

```{r}
# select the best one (simpler, conservative model)
show_best(xgb_res.new, 'rmse')
best_param.new <- select_best(xgb_res.new, 'rmse')

# finalize workflow
final_xgb.new <- finalize_workflow(xgb_wf.new, best_param.new)
```

```{r}
# variable importance plot
final_xgb.new %>% 
    fit(data = train.new) %>%
    pull_workflow_fit() %>% # if you want the actual fit
    vip(geom = 'point')
# hour (dominant), newtemp, humidity, workingday, day, windspeed
```

```{r}
# update the workflow with the final model
final_model.new <- finalize_model(xgb_spec, best_param.new) 
final_workflow.new <- xgb_wf.new %>% update_model(final_model.new)
xgb_fit.new <- fit(final_workflow.new, data = train.new)
```

```{r}
# make a prediction
pred_xgb.new <- 
    predict(xgb_fit.new, test.new) %>% 
    mutate(modelo = "XGBoost",
           .predexp = exp(.pred))

# we should have positive predictions
expect_gt(min(pred_xgb.new$.predexp), -0.001, "we have negative counts")
```

```{r}
# submission
subfile <- read_csv(file.path(file_path, "sampleSubmission.csv"))
subfile$count <- pred_xgb.new$.predexp
write.csv(subfile, row.names = FALSE,
          "../minhee/xgboost_new_scale.csv")

# Last week xgboost : 0.50121 
# this week xgboost : 0.47274 
# separate models : 0.44918
# 1. changes: (step_corr(all_numeric(), threshold = .9) -> atemp removed): 0.46577
# 2. changes: (1) + (lagged count included): 1.02
# 2-2. changes: (1) + (lagged hour included at time 1 and 2): 0.47
# 3. changes: new.temp = (atemp + temp)/2, remove temp and atemp: 0.436
# 4. changes: (3) + (scale weather-related vars): 0.46537
# 5. (4) + separate model: 0.46065
# 6. (4) + predict count^(1/4) instead of log(count+1): 0.47149

```

<!--

```{r}
registered_recipe_new <- all_data %>% 
    recipe(registered~.) %>% 
    #step_lag(count, lag=1) %>% # lag count
    #step_corr(all_numeric(), threshold = .9)
    #step_lag(hour, lag=1:2) %>% # lag hour
    step_mutate(
    new.temp = (temp + atemp)/2) %>%
    step_rm(datetime, year, temp, atemp,casual, count) %>%  # remove atemp, temp, and include new.temp
    step_scale(windspeed, humidity, new.temp) %>% # scale
    step_log(registered, offset = 1) %>% 
    step_dummy(all_nominal(),-all_outcomes()) %>%
    step_nzv(all_numeric(), -all_outcomes()) %>%
    prep(training = all_data)

# split data
registered_all_data2 <- juice(registered_recipe_new)
registered_train2 <- registered_all_data2[train_index,]
registered_test2 <- registered_all_data2[-train_index,]

# set grid
xgb_grid_registered2 <- grid_latin_hypercube(
    tree_depth(), 
    min_n(), 
    loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), registered_train2),
    learn_rate(), 
    size = 30
)

# set workflow
xgb_wf_registered2 <- workflow() %>% 
    add_formula(registered~.) %>% 
    add_model(xgb_spec)
```

```{r}
# tuning
set.seed(1234)
vb_folds_registered2 <- vfold_cv(registered_train2, v = 5, strata = registered)
vb_folds_registered2

doParallel::registerDoParallel()

xgb_res_registered2 <- tune_grid(
    xgb_wf_registered2,
    resamples = vb_folds_registered2, 
    grid = xgb_grid_registered2,
    control = control_grid(save_pred = TRUE)  
)
```

```{r}
# visualize results
xgb_res_registered2 %>% 
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry: sample_size,
               names_to="parameter",
               values_to="value") %>%
  ggplot(aes(value, mean, color=parameter)) +
  geom_point(show.legend=F) +
  facet_wrap(~parameter, scales="free_x")
```

```{r}
# select the best one
show_best(xgb_res_registered2, 'rmse')
best_param_registered2 <- select_best(xgb_res_registered2, 'rmse')

# finalize workflow
final_xgb_registered2 <- finalize_workflow(xgb_wf_registered2, best_param_registered2)

```

```{r}
# draw variable importance plot
final_xgb_registered2 %>% 
    fit(data = registered_train2) %>% 
    pull_workflow_fit() %>%
    vip(geom = 'point')
# hour (dominant), newtemp, humidity, workingday, day, windspeed
```

```{r}
# update the workflow with the final model
final_model_registered2 <- finalize_model(xgb_spec, best_param_registered2) 
final_workflow_registered2 <- xgb_wf_registered2 %>% update_model(final_model_registered2)
xgb_fit_registered2 <- fit(final_workflow_registered2, data = registered_train2)
```

```{r}
# make a prediction
pred_xgb_registered2 <- 
    predict(xgb_fit_registered2, registered_test2) %>% 
    mutate(modelo = "XGBoost",
           .predexp = exp(.pred))

# we should not have negative predictions
expect_gt(min(pred_xgb_registered2$.predexp), -0.001, "we have negative counts")
```

```{r}
# same procedure for causal model
casual_recipe_new <- all_data %>% 
    recipe(casual~.) %>% 
    #step_lag(count, lag=1) %>% # lag count
    #step_corr(all_numeric(), threshold = .9)
    #step_lag(hour, lag=1:2) %>% # lag hour
    step_mutate(
    new.temp = (temp + atemp)/2) %>%
    step_rm(datetime, year, temp, atemp,registered, count) %>%  # remove atemp, temp, and include new.temp
    step_scale(windspeed, humidity, new.temp) %>% # scale
    step_log(casual, offset = 1) %>% 
    step_dummy(all_nominal(),-all_outcomes()) %>%
    step_nzv(all_numeric(), -all_outcomes()) %>%
    prep(training = all_data)

# split data
casual_all_data2 <- juice(casual_recipe_new)
casual_train2 <- casual_all_data2[train_index,]
casual_test2 <- casual_all_data2[-train_index,]

# set grid
xgb_grid_casual2 <- grid_latin_hypercube(
    tree_depth(), 
    min_n(), 
    loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), casual_train2),
    learn_rate(), 
    size = 30
)

# set workflow
xgb_wf_casual2 <- workflow() %>% 
    add_formula(casual~.) %>% 
    add_model(xgb_spec)
```

```{r}
set.seed(1234)
vb_folds_casual2 <- vfold_cv(casual_train2, v = 5, strata = casual)
vb_folds_casual2

doParallel::registerDoParallel()

# tuning
xgb_res_casual2 <- tune_grid(
    xgb_wf_casual2, 
    resamples = vb_folds_casual2, 
    grid = xgb_grid_casual2, 
    control = control_grid(save_pred = TRUE)   
)
```

```{r}
# visualize results
xgb_res_casual2 %>% 
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry: sample_size,
               names_to="parameter",
               values_to="value") %>%
  ggplot(aes(value, mean, color=parameter)) +
  geom_point(show.legend=F) +
  facet_wrap(~parameter, scales="free_x")
```

```{r}
# select the best model
show_best(xgb_res_casual2, 'rmse')
best_param_casual2 <- select_best(xgb_res_casual2, 'rmse')

# finalize workflow
final_xgb_casual2 <- finalize_workflow(xgb_wf_casual2, best_param_casual2)
```

```{r}
# draw variable importance plot
final_xgb_casual2 %>% 
    fit(data = casual_train2) %>%
    pull_workflow_fit() %>%
    vip(geom = 'point')
# hour (dominant), newtemp, humidity, workingday, day, windspeed, weather -rain
```

```{r}
# update the workflow with the final model
final_model_casual2 <- finalize_model(xgb_spec, best_param_casual2) 
final_workflow_casual2 <- xgb_wf_casual2 %>% update_model(final_model_casual2)
xgb_fit_casual2 <- fit(final_workflow_casual2, data = casual_train2)
```

```{r}
# make a prediction
pred_xgb_casual2 <- 
    predict(xgb_fit_casual2, casual_test2) %>% 
    mutate(modelo = "XGBoost",
           .predexp = exp(.pred))

# we should not have negative predictions
expect_gt(min(pred_xgb_casual2$.predexp), -0.001, "we have negative counts")

```

```{r}
subfile <- read_csv(file.path(file_path, "sampleSubmission.csv"))
subfile$count
subfile$count <- (pred_xgb_casual2$.predexp + pred_xgb_registered2$.predexp)

write.csv(subfile, row.names = FALSE,
          "../minhee/xgboost_separate_scale.csv")
```
-->


<!--

```{r}
train <- read_csv(file.path(file_path, "train.csv")) # 1-19th
test <- read_csv(file.path(file_path, "test.csv")) # predict rests 9-12 days
c("registered", "causal", "count") %in% names(test)

train$count <- (train$count)^(1/4)
all_data <- bind_rows(train, test)

all_data <- all_data %>%
   mutate(year = year(datetime),
          month = month(datetime),
          wday = wday(datetime),
          day = day(datetime), 
          hour = hour(datetime),
          across(c(workingday, holiday, month, weather, season, wday), as.factor),
          season = factor(season, labels = c('winter', 'spring', 'summer', 'fall')),
          wday = factor(wday, labels = c('Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday')),
          weather = factor(weather, labels = c('clear', 'cloudy', 'bad', 'worse')),
          weather = fct_collapse(weather,rain = c("bad", "worse")))

# set recipe
bike_res_new_power <- all_data %>% 
    recipe(count~.) %>% 
    #step_lag(count, lag=1) %>% # lag count
    #step_corr(all_numeric(), threshold = .9)
    #step_lag(hour, lag=1:2) %>% # lag hour
    step_mutate(
    new.temp = (temp + atemp)/2) %>%
    step_rm(datetime, year, temp, atemp,casual, registered) %>%  # remove atemp, temp, and include new.temp
    step_scale(windspeed, humidity, new.temp) %>% # scale
    #step_log(count, offset = 1) %>% 
    step_dummy(all_nominal(),-all_outcomes()) %>%
    step_nzv(all_numeric(), -all_outcomes()) %>%
    prep(training = all_data)


# split data
all_data4 <- juice(bike_res_new_power)
train.new.power <- all_data4[train_index,]
test.new.power <- all_data4[-train_index,]

# set grid
xgb_grid.new.power <- grid_latin_hypercube(
    tree_depth(), min_n(), loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), train.new.power),
    learn_rate(), 
    size = 30
)

# set workflow
xgb_wf.new.power <- workflow() %>% 
    add_formula(count~.) %>% 
    add_model(xgb_spec)
```

```{r}
set.seed(1234)
vb_folds_new.power <- vfold_cv(train.new.power, v = 5, strata = count)
vb_folds_new.power

# tuning
doParallel::registerDoParallel()

xgb_res.new.power <- tune_grid(
    xgb_wf.new.power,
    resamples = vb_folds_new.power, 
    grid = xgb_grid.new.power,
    control = control_grid(save_pred = TRUE)  
)
```

```{r}
# visualize results
xgb_res.new.power %>% 
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry: sample_size,
               names_to="parameter",
               values_to="value") %>%
  ggplot(aes(value, mean, color=parameter)) +
  geom_point(show.legend=FALSE) +
  facet_wrap(~parameter, scales="free_x")
```

```{r}
# select the best one (simpler, conservative model)
show_best(xgb_res.new.power, 'rmse')
best_param.new.power <- select_best(xgb_res.new.power, 'rmse')

# finalize workflow
final_xgb.new.power <- finalize_workflow(xgb_wf.new.power, best_param.new.power)
```

```{r}
# variable importance plot
final_xgb.new.power %>% 
    fit(data = train.new.power) %>%
    pull_workflow_fit() %>% # if you want the actual fit
    vip(geom = 'point')
# hour (dominant), newtemp, workingday, humidity, day, season fall, windspeed
```

```{r}
# update the workflow with the final model
final_model.new.power <- finalize_model(xgb_spec, best_param.new.power) 
final_workflow.new.power <- xgb_wf.new %>% update_model(final_model.new.power)
xgb_fit.new.power <- fit(final_workflow.new.power, data = train.new.power)
```

```{r}
# make a prediction
pred_xgb.new.power <- 
    predict(xgb_fit.new.power, test.new.power) %>% 
    mutate(modelo = "XGBoost",
           .predpower = (.pred)^4)

# we should have positive predictions
expect_gt(min(pred_xgb.new.power$.predpower), -0.001, "we have negative counts")
```

```{r}
# submission
subfile <- read_csv(file.path(file_path, "sampleSubmission.csv"))
subfile$count <- pred_xgb.new.power$.predpower
write.csv(subfile, row.names = FALSE,
          "../minhee/xgboost_new_scale_power.csv")
```

-->

## Separate model 3

```{r}
registered_recipe_new <- all_data %>% 
    recipe(registered~.) %>% 
    step_mutate(
    new.temp = (temp + atemp)/2) %>%
    step_rm(datetime, year, temp, atemp,casual, count, day) %>%  # remove atemp, temp, and include new.temp
    step_scale(windspeed, humidity, new.temp) %>% # scale
    step_log(registered, offset = 1) %>% 
    step_dummy(all_nominal(),-all_outcomes()) %>%
    step_nzv(all_numeric(), -all_outcomes()) %>%
    prep(training = all_data)

# split data
registered_all_data2 <- juice(registered_recipe_new)
registered_train2 <- registered_all_data2[train_index,]
registered_test2 <- registered_all_data2[-train_index,]

# set grid
xgb_grid_registered2 <- grid_latin_hypercube(
    tree_depth(), 
    min_n(), 
    loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), registered_train2),
    learn_rate(), 
    size = 40
)

# set workflow
xgb_wf_registered2 <- workflow() %>% 
    add_formula(registered~.) %>% 
    add_model(xgb_spec)
```

```{r}
# tuning
set.seed(1234)
vb_folds_registered2 <- vfold_cv(registered_train2, v = 10, strata = registered)
vb_folds_registered2

doParallel::registerDoParallel()

xgb_res_registered2 <- tune_grid(
    xgb_wf_registered2,
    resamples = vb_folds_registered2, 
    grid = xgb_grid_registered2,
    control = control_grid(save_pred = TRUE)  
)
```

```{r}
# visualize results
xgb_res_registered2 %>% 
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry: sample_size,
               names_to="parameter",
               values_to="value") %>%
  ggplot(aes(value, mean, color=parameter)) +
  geom_point(show.legend=F) +
  facet_wrap(~parameter, scales="free_x")
```

```{r}
# select the best one
show_best(xgb_res_registered2, 'rmse')
best_param_registered2 <- select_best(xgb_res_registered2, 'rmse')

# finalize workflow
final_xgb_registered2 <- finalize_workflow(xgb_wf_registered2, best_param_registered2)

```

```{r}
# draw variable importance plot
final_xgb_registered2 %>% 
    fit(data = registered_train2) %>% 
    pull_workflow_fit() %>%
    vip(geom = 'point')
# hour (dominant), newtemp, humidity, workingday, day, windspeed
```

```{r}
# update the workflow with the final model
final_model_registered2 <- finalize_model(xgb_spec, best_param_registered2) 
final_workflow_registered2 <- xgb_wf_registered2 %>% update_model(final_model_registered2)
xgb_fit_registered2 <- fit(final_workflow_registered2, data = registered_train2)
```

```{r}
# make a prediction
pred_xgb_registered2 <- 
    predict(xgb_fit_registered2, registered_test2) %>% 
    mutate(modelo = "XGBoost",
           .predexp = exp(.pred))

# we should not have negative predictions
expect_gt(min(pred_xgb_registered2$.predexp), -0.001, "we have negative counts")
```

```{r}
# same procedure for causal model
casual_recipe_new <- all_data %>% 
    recipe(casual~.) %>% 
    step_rm(datetime, year, temp,registered, count) %>%  # use atemp for casual
    step_scale(windspeed, humidity, atemp) %>% # scale
    step_log(casual, offset = 4) %>% 
    step_dummy(all_nominal(),-all_outcomes()) %>%
    step_nzv(all_numeric(), -all_outcomes()) %>%
    prep(training = all_data)

# split data
casual_all_data2 <- juice(casual_recipe_new)
casual_train2 <- casual_all_data2[train_index,]
casual_test2 <- casual_all_data2[-train_index,]

# set grid
xgb_grid_casual2 <- grid_latin_hypercube(
    tree_depth(), min_n(),  loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), casual_train2),learn_rate(), 
    size = 30
)

# set workflow
xgb_wf_casual2 <- workflow() %>% 
    add_formula(casual~.) %>% 
    add_model(xgb_spec)
```

```{r}
set.seed(1234)
vb_folds_casual2 <- vfold_cv(casual_train2, v = 10, strata = casual)
vb_folds_casual2

doParallel::registerDoParallel()

# tuning
xgb_res_casual2 <- tune_grid(
    xgb_wf_casual2, 
    resamples = vb_folds_casual2, 
    grid = xgb_grid_casual2, 
    control = control_grid(save_pred = TRUE)   
)
```

```{r}
# visualize results
xgb_res_casual2 %>% 
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry: sample_size,
               names_to="parameter",
               values_to="value") %>%
  ggplot(aes(value, mean, color=parameter)) +
  geom_point(show.legend=F) +
  facet_wrap(~parameter, scales="free_x")
```

```{r}
# select the best model
show_best(xgb_res_casual2, 'rmse')
best_param_casual2 <- select_best(xgb_res_casual2, 'rmse')

# finalize workflow
final_xgb_casual2 <- finalize_workflow(xgb_wf_casual2, best_param_casual2)
```

```{r}
# draw variable importance plot
final_xgb_casual2 %>% 
    fit(data = casual_train2) %>%
    pull_workflow_fit() %>%
    vip(geom = 'point')
# hour (dominant), atemp, humidity, workingday, day, summer, windspeed
```

```{r}
# update the workflow with the final model
final_model_casual2 <- finalize_model(xgb_spec, best_param_casual2) 
final_workflow_casual2 <- xgb_wf_casual2 %>% update_model(final_model_casual2)
xgb_fit_casual2 <- fit(final_workflow_casual2, data = casual_train2)
```

```{r}
# make a prediction
pred_xgb_casual2 <- 
    predict(xgb_fit_casual2, casual_test2) %>% 
    mutate(modelo = "XGBoost",
           .predexp = exp(.pred))

# we should not have negative predictions
expect_gt(min(pred_xgb_casual2$.predexp), -0.001, "we have negative counts")

```

```{r}
subfile <- read_csv(file.path(file_path, "sampleSubmission.csv"))
subfile$count
subfile$count <- (pred_xgb_casual2$.predexp + pred_xgb_registered2$.predexp)

write.csv(subfile, row.names = FALSE,
          "../minhee/xgboost_separate_scale2.csv")
```
-->








