---
title: "Stack을 해보자 뚠둔"
date: '`r Sys.Date()`'
output: 
  html_document:
    toc: true
    toc_float: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r, message = FALSE, warning=FALSE}
library(parallel)
library(doParallel)
registerDoParallel(cores = 8)
library(stacks)
library(tidyverse)
library(tidymodels)
```

## Overviews

1. Stacking 알아보기

[`{stacks}`](https://stacks.tidymodels.org/)의 소개에 따르면, `{stack}`은 여러 모델의
결과를 앙상블 방법으로 취합하여 각 모델에 따른 예측값을 생성하는 새로운 모델을 만드는
방법을 의미한다.

  + 기존의 방식이  training data로 학습한 모델을 이용하여 test 데이터의 자료로 
    예측하는 것이라면
  
  + 이 방법은 개별 모델이 예측한 데이터를 다시 training data로 이용해서 
    학습한다는 것이라고 볼 수 있다.
  
  + 즉, training -> 여러 개의 모델로 학습 -> test prediction
    -> 이걸 다시 training 처럼 사용 -> 최종 모델을 하나 선택해서 (rmse 등 고려) 학습
    -> 최종 prediction 생산

2. Stacking process

  1. 먼저 각각의 사용할 모델에 적합한 레시피를 작성한다.
  
  2. Pre-processing
  
  3. Stacking을 위하여 각 모델의 예측값을 저장할 tuning control을 정의해준다.

  4. Random forest랑 xgboost 모델 같은 경우는 grid를 지정
  
  5. Workflow를 지정해준다.
  
  6. 모델을 fitting 후 predictions를 collect 해준다.
  
  7. 그 collected predictions를 이용해 다시 prediction?
  

  
## Import the data

```{r}
file_path <- "../input/bike-sharing-demand"
files <- list.files(file_path)
files
```

## Train and Test sets
```{r}
train <- read_csv(file.path(file_path, "train.csv"))
train <- train %>% select(-casual, -registered)
test <- read_csv(file.path(file_path, "test.csv"))
all_data <- bind_rows(train, test)
```


```{r}
set.seed(1234)

# Use train data as our validation k-folds data set.
k_folds_data <- vfold_cv(train)
```

```{r}
train
```

## Main approaches for ensemble modeling

```{r}
# pca regression model: Linear regression model using pca as a preprocessor
pca_rec <- train %>% 
  recipe(count~.) %>%
  step_rm(datetime) %>%
  step_log(count, offset = 1) %>% 
  # remove predictors that basically have no variants in it.
  step_nzv(all_predictors()) %>%
  # remove any highly correlated variables
  step_corr(all_numeric(), -all_outcomes()) %>% 
  # step linear combinations
  step_lincomb(all_numeric(), -all_outcomes()) %>%
  # convert other variables to nominal
  step_other(all_nominal()) %>%
  # normalize all numeric variables without outcoems.
  step_normalize(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal()) %>%
  # pca using five components.
  step_pca(all_predictors(), num_comp = 5)

# Let's see the prep()
pca_rec %>% prep()

# Spline model
spline_rec <- train %>% 
  recipe(count~.) %>%
  step_log(count, offset = 1) %>% 
  step_nzv(all_predictors()) %>%
  step_corr(all_numeric(), -all_outcomes()) %>% 
  step_lincomb(all_numeric(), -all_outcomes()) %>%
  # only address numeric data
  step_rm(datetime, all_nominal()) %>%
  step_bs(all_predictors()) %>%
  step_YeoJohnson(all_predictors())

spline_rec %>% prep()

tidy_rec <-  train %>% 
  recipe(count~.) %>%
  step_rm(datetime) %>%
  step_nzv(all_predictors()) %>%
  step_corr(all_numeric(), -all_outcomes()) %>% 
  step_lincomb(all_numeric(), -all_outcomes()) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal())

tidy_rec %>% prep()
```
```{r}
train2 <- juice(tidy_rec %>% prep())
```

# Define pre-processing
```{r}
pca_regression_model <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

spline_model <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

randomForest_model <- 
  rand_forest(min_n = tune(), trees = tune()) %>%
  set_mode("regression") %>%
  set_engine("randomForest")

xgboost_model <- 
  boost_tree(learn_rate = tune(), trees = tune(), tree_depth = tune()) %>%
  set_mode("regression") %>%
  set_engine("xgboost")

# just copied from Dondon
xgboost_model <- boost_tree(
    trees = 1000, # 앙상블에 포함되는 tree의 수 
    tree_depth = tune(), # 얼마만큼 노드를 split할건지 
    min_n = tune(), # 노드를 분할하는데 필요한 최소 데이터의 수
    loss_reduction = tune(), # 노드 분할에 필요한 loss의 감소량 
    sample_size = tune(), # The amount of data exposed to the fitting routine
    mtry = tune(), # The number of predictors that will be randomly sampled at each split when creating the tree models. 
    learn_rate = tune() 
) %>% 
  set_mode('regression') %>%
  set_engine('xgboost')
```

# Define Tuning Control
```{r}
# Save predictions from each model for stacking
model_control <- control_grid(save_pred = T, save_workflow = T)
model_metrics <- metric_set(rmse, mae, rsq)

# Define Grids
randforest_grid <- grid_regular(parameters(randomForest_model), 
                                levels = 3, filter = c(trees > 1))

#xgboost_grid <- grid_regular(parameters(xgboost_model), 
#                             levels = 3, filter = c(trees > 1))
# copied from Dondon
xgboost_grid <- grid_latin_hypercube(
  tree_depth(), min_n(), loss_reduction(), 
  sample_size = sample_prop(), 
  finalize(mtry(), train), # mtry() : [1, ?], finalize(mtry(), train2) : [1, 30]
  learn_rate(), 
  size = 30
)
```

## Define workflows

```{r}
pca_wf <- workflow() %>%
  add_model(pca_regression_model) %>%
  add_recipe(pca_rec)

spline_wf <- workflow() %>%
  add_model(spline_model) %>%
  add_recipe(spline_rec)

randomForest_wf <- workflow() %>%
  add_model(randomForest_model) %>%
  add_recipe(tidy_rec)

xgboost_wf <- workflow() %>%
  add_model(xgboost_model) %>%
  add_recipe(tidy_rec)
```

# Fit models

```{r}
pca_res <- fit_resamples(
  pca_wf,
  resamples = k_folds_data,
  matrics = model_metrics,
  control = model_control
)

spline_res <- fit_resamples(
  spline_wf,
  resamples = k_folds_data,
  matrics = model_metrics,
  control = model_control
)

# 엄청 오래 걸림
randomForest_res <- tune_grid(
  randomForest_wf,
  resamples = k_folds_data,
  grid = randforest_grid,
  matrics = model_metrics,
  control = model_control
)

# 이거도 짧지는 않음
xgboost_res <- tune_grid(
  xgboost_wf,
  resamples = k_folds_data,
  grid = xgboost_grid,
  matrics = model_metrics,
  control = model_control
)

data <- ls()
saveRDS(data, "stacking.rds")
```

```{r}
## It does not work because of xgboost_res (has no .row column)

bike_stack <- stacks() %>%
  add_candidates(pca_res) %>%`
  add_candidates(spline_res)# %>%
#  add_candidates(randomForest_res) %>%
#  add_candidates(xgboost_res)

bike_stack <- bike_stack %>%
  blend_predictions() %>%
  fit_members()

predict(bike_stack, test) %>%
 bind_cols(test %>% select(count)) %>%
  model_metrics(truth = count, estimate = .pred) %>%
  pivot_wider(name_from = .metric, values_from = .estimate)
```

3. 하면서 여전히 의문인점

  1. 원래 이렇게 randomForest는 오래 걸리는 것인가요...?
  
  2. stacks 의 sequence를 살펴보기 위해 recipe에 다른 내용은 많이 포함시키지
     않았는데, 이런 것들을 다 고려해서 수정하고 반영한다고 할 때, 학습이 이렇게
     오래 걸리는 게 정상인지?
     
  3. xgboost 모델에서 설정하는 parameters들은 대개 dondon님꺼를 참고했는데
     xgboost에 대해 자세히 모르니 이해가 잘 안감. (공부 필요, 논문 or 책 추천?)

  4. Stacks의 사용하여 얻을 수 있는 이익과 비용은 무엇일까?
  
  5. Bikesharing 같은 경우 casual, registered를 먼저 예측하고 나중에 그 예측값을
     더하는 것과 그냥 count로 예측하는 것, 어느게 더 나을까?


<!-- 시행착오? 아니면 여전히 착오...?
## Use stacks package for xgboost
```{r}
# xgboost는 아까 stacks()로 안되었으니까 manually하게 합쳐보자.

randomForest_final_param <- 
  randomForest_res %>% 
  show_best("rmse") %>%
  slice(1) %>%
  select(trees, min_n)

xgboost_final_param <- 
  xgboost_res %>%
  show_best("rmse") %>%
  slice(1) %>%
  select(learn_rate) %>% mutate(trees = 1000)
```

# collect model predictions to stack
```{r}
xgboost_stack <- 
  xgboost_res %>%
  collect_predictions() %>%
  inner_join(xgboost_final_param) %>%
  select(id, .row, count, xgboost = .pred)

randomForest_stack <- 
  randomForest_res %>%
  collect_predictions() %>%
  inner_join(randomForest_final_param) %>%
  select(id, .row, count,  randomForest = .pred)

pca_stack <- pca_res %>%
  collect_predictions() %>%
  select(id, .row, pca = .pred)

spline_stack <- spline_res %>%
  collect_predictions() %>%
  select(id, .row, spline = .pred)


# Create ensamble data

stack_df <- xgboost_stack %>%
  left_join(randomForest_stack) %>%
  left_join(pca_stack) %>%
  left_join(spline_stack) %>%
  select(-id, -.row)

stack_model <- 
  linear_reg(penalty = .5, mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet") %>%
  fit(count~., data = stack_df)

stack_model %>% tidy()

```

# Finalize submodels

```{r}
xgboost_wf <- xgboost_wf %>%
  finalize_workflow(xgboost_final_param) %>%
  fit(a)

randomForest_wf <- randomForest_wf %>%
  finalize_workflow(randomForest_final_param) %>%
  last_fit(test)

pca_wf <- pca_wf %>%
  last_fit(test)

spline_wf <- spline_wf %>% last_fit(test)
```

# Extract Predictions from Submodels

```{r}
stack_final_df <- 
  tibble("model" = list(xgboost_wf, randomForest_wf, pca_wf, spline_wf),
       "model_names" = c("xgboost", "randomForest", "pca", "spline")) %>%
  mutate(pred = map(model, collect_predictions))

stack_final_df <- stack_final_df %>%
  select(model_names, pred) %>%
  unnest(pred) %>%
  pivot_wider(names_from = model_names, values_from = .pred) %>%
  select(-id, -.row)

predict(stack_model, stack_final_df) %>%
  bind_cols(stack_final_df) %>%
  rename("stack" = .pred) %>%
  pivot_longer(-count) %>%
  group_by(name) %>%
  model_metrics(truth = count, estimate = value) %>%
  ungroupo() %>%
  pivot_wider(names_from = .metric, values_from = .estimate) %>%
  arrange(rmse)
  
```

```{r}

predict(bike_stack, tidy_test) %>%
  bind_cols(tidy_test %>% select(count)) %>%
  model_metrics(truth = count, estimate = .pred) %>%
  pivot_wider(name_from = .metric, values_from = .estimate)
```
-->

```{r}
# submission
subfile <- read_csv(file.path(file_path, "sampleSubmission.csv"))
#subfile$count <- 
#write.csv(subfile, row.names = FALSE,
#          "stacking.csv")

```





